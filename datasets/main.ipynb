{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened\n"
     ]
    }
   ],
   "source": [
    "from groups import update_groups_dataset\n",
    "\n",
    "update_groups_dataset(105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='20И6794' fio='Пономарев Владилен Игнатьевич' group='ИЭБО-54-22' financing='Б' gender='М' permanent_registration='клх Кинешма, наб. Мелиораторов, д. 429 стр. 856, 107691'\n"
     ]
    }
   ],
   "source": [
    "from random import choice, randint\n",
    "\n",
    "from students import Student\n",
    "from groups import get_groups\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker(\"ru_RU\")\n",
    "\n",
    "groups = get_groups()\n",
    "# first value in 'Название' column\n",
    "group_name = groups.iloc[0, 0]\n",
    "\n",
    "student_id = f\"{randint(20, 23)}И{randint(1, 9999):04d}\"\n",
    "gender = choice(\"МЖ\")\n",
    "print(Student(\n",
    "            id=student_id,\n",
    "            fio=faker.name_male() if gender == \"М\" else faker.name_female(),\n",
    "            group=group_name,\n",
    "            financing=choice([\"Б\", \"К\"]),\n",
    "            gender=gender,\n",
    "            permanent_registration=faker.address(),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ИНБО-31-22': 30, 'ИВБО-94-22': 30, 'ИКБО-26-23': 30, 'ИЭБО-47-20': 30, 'ИНБО-38-21': 30, 'ИМБО-53-21': 30, 'ИЭБО-54-21': 30, 'ИМБО-60-21': 30, 'ИЛБО-30-21': 30, 'ИКБО-33-22': 30, 'ИВБО-25-20': 30, 'ИКБО-96-20': 30, 'ИМБО-93-20': 30, 'ИМБО-33-21': 30, 'ИКБО-25-22': 30, 'ИМБО-29-22': 30, 'ИЭБО-15-22': 30, 'ИВБО-33-22': 30, 'ИМБО-77-22': 30, 'ИЛБО-64-22': 30, 'ИЛБО-61-20': 30, 'ИМБО-05-21': 30, 'ИЛБО-10-21': 30, 'ИКБО-44-22': 30, 'ИКБО-70-20': 30, 'ИМБО-59-21': 30, 'ИЛБО-11-23': 30, 'ИНБО-42-20': 30, 'ИЭБО-26-20': 30, 'ИКБО-44-21': 30, 'ИНБО-49-21': 30, 'ИЭБО-44-23': 30, 'ИЭБО-14-22': 30, 'ИВБО-05-21': 30, 'ИНБО-05-20': 30, 'ИВБО-47-21': 30, 'ИКБО-99-23': 30, 'ИМБО-13-22': 30, 'ИВБО-52-20': 30, 'ИВБО-04-21': 30, 'ИМБО-95-22': 30, 'ИЛБО-71-21': 30, 'ИМБО-52-22': 30, 'ИКБО-38-21': 30, 'ИЭБО-79-21': 30, 'ИЛБО-44-20': 30, 'ИНБО-13-23': 30, 'ИКБО-03-21': 30, 'ИНБО-52-20': 30, 'ИНБО-05-23': 30, 'ИКБО-53-20': 30, 'ИВБО-60-23': 30, 'ИМБО-27-23': 30, 'ИВБО-11-21': 30, 'ИЛБО-51-21': 30, 'ИКБО-55-22': 30, 'ИКБО-13-23': 30, 'ИКБО-54-20': 30, 'ИЭБО-38-22': 30, 'ИЛБО-73-21': 30, 'ИЛБО-85-21': 30, 'ИНБО-24-23': 30, 'ИЛБО-37-22': 30, 'ИКБО-22-23': 30, 'ИЛБО-06-22': 30, 'ИМБО-07-20': 30, 'ИМБО-19-23': 30, 'ИМБО-33-23': 30, 'ИКБО-32-22': 30, 'ИЛБО-36-23': 30, 'ИЭБО-40-20': 30, 'ИЛБО-64-21': 30, 'ИЭБО-93-21': 30, 'ИВБО-28-23': 30, 'ИНБО-70-23': 30, 'ИКБО-89-22': 30, 'ИЭБО-74-21': 30, 'ИМБО-34-22': 30, 'ИМБО-50-23': 30, 'ИКБО-01-22': 30}\n"
     ]
    }
   ],
   "source": [
    "from students import get_students\n",
    "from groups import get_groups\n",
    "from config import load_config\n",
    "import pandas as pd\n",
    "\n",
    "actual_groups: pd.DataFrame = get_groups()\n",
    "group_names = actual_groups[\"Название\"].unique()\n",
    "\n",
    "actual_students: pd.DataFrame = get_students()\n",
    "students_in_groups = actual_students.groupby(\"Группа\").size().to_dict()\n",
    "\n",
    "students_to_generate = {\n",
    "    group_name: load_config().students_per_group - students_in_groups.get(group_name, 0)\n",
    "    for group_name in group_names\n",
    "}\n",
    "print(students_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Название', 'Специальность']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenList([None])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from groups import Group\n",
    "\n",
    "print([f.title for f in Group.model_fields.values()])\n",
    "\n",
    "pd.DataFrame(columns=[f.title for f in Group.model_fields.values()]).columns.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Student(id='23И9812', fio='Панов Никандр Вилорович', group='ИНБО-31-22', financing='Б', gender='М', permanent_registration_city='Краснодар')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from students import generate_students\n",
    "\n",
    "generate_students([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Превышено количество попыток генерации уникального личного номера студента",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstudents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m update_students_dataset\n\u001b[1;32m----> 3\u001b[0m \u001b[43mupdate_students_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\VS Code Projects\\big-data-6-sem\\datasets-generation\\students\\__init__.py:101\u001b[0m, in \u001b[0;36mupdate_students_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\VS Code Projects\\big-data-6-sem\\datasets-generation\\students\\__init__.py:88\u001b[0m, in \u001b[0;36mgenerate_students\u001b[1;34m(used_student_ids)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m students_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     students: Generator[Student, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m students_generator(\n\u001b[0;32m     87\u001b[0m         group_name, used_student_ids\n\u001b[1;32m---> 88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m     generated_students\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mnext\u001b[39m(students) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(students_count)])\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_students\n",
      "File \u001b[1;32mc:\\VS Code Projects\\big-data-6-sem\\datasets-generation\\students\\__init__.py:50\u001b[0m, in \u001b[0;36mstudents_generator\u001b[1;34m(group_name, used_student_ids)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m     student_id \u001b[38;5;241m:=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandint(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m23\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mИ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m9999\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m used_student_ids:\n\u001b[0;32m     49\u001b[0m     attempt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПревышено количество попыток генерации уникального личного номера студента\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         )\n\u001b[0;32m     54\u001b[0m used_student_ids\u001b[38;5;241m.\u001b[39mappend(student_id)\n",
      "\u001b[1;31mValueError\u001b[0m: Превышено количество попыток генерации уникального личного номера студента"
     ]
    }
   ],
   "source": [
    "from students import update_students_dataset\n",
    "\n",
    "update_students_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidDataError",
     "evalue": "The provided data does not match the metadata:\nThe columns ['lesson_1', 'lesson_10', 'lesson_11', 'lesson_12', 'lesson_2', 'lesson_3', 'lesson_4', 'lesson_5', 'lesson_6', 'lesson_7', 'lesson_8', 'lesson_9'] are not present in the metadata.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidDataError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m     synthetic_data \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39msample(num_sequences)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_data\n\u001b[1;32m---> 46\u001b[0m synthetic_data_increasing \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mincreasing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# synthetic_data_fluctuating = generate_synthetic_data(\"fluctuating\", 16, 100)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# synthetic_data_steady = generate_synthetic_data(\"steady\", 16, 100)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# synthetic_data_random = generate_synthetic_data(\"random\", 16, 100)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mgenerate_synthetic_data\u001b[1;34m(pattern, sequence_length, num_sequences)\u001b[0m\n\u001b[0;32m     38\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(sequence \u001b[38;5;241m*\u001b[39m (sequence_length \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence)))\n\u001b[0;32m     40\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlesson_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length)])\n\u001b[1;32m---> 41\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39msample(num_sequences)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_data\n",
      "File \u001b[1;32mc:\\Users\\sasha\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\datasets-generation-iKdB8DkJ-py3.12\\Lib\\site-packages\\sdv\\single_table\\base.py:413\u001b[0m, in \u001b[0;36mBaseSynthesizer.fit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_processor\u001b[38;5;241m.\u001b[39mreset_sampling()\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_processed_data(processed_data)\n",
      "File \u001b[1;32mc:\\Users\\sasha\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\datasets-generation-iKdB8DkJ-py3.12\\Lib\\site-packages\\sdv\\single_table\\ctgan.py:257\u001b[0m, in \u001b[0;36mCTGANSynthesizer._preprocess\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_processor\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_warning(data)\n",
      "File \u001b[1;32mc:\\Users\\sasha\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\datasets-generation-iKdB8DkJ-py3.12\\Lib\\site-packages\\sdv\\single_table\\base.py:167\u001b[0m, in \u001b[0;36mBaseSynthesizer.validate\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate data.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m                * values of a column don't satisfy their sdtype\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_constraints(data)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# Retaining the logic of returning errors and raising them here to maintain consistency\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# with the existing workflow with synthesizers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sasha\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\datasets-generation-iKdB8DkJ-py3.12\\Lib\\site-packages\\sdv\\single_table\\base.py:127\u001b[0m, in \u001b[0;36mBaseSynthesizer._validate_metadata\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    124\u001b[0m     errors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39merrors\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidDataError(errors)\n",
      "\u001b[1;31mInvalidDataError\u001b[0m: The provided data does not match the metadata:\nThe columns ['lesson_1', 'lesson_10', 'lesson_11', 'lesson_12', 'lesson_2', 'lesson_3', 'lesson_4', 'lesson_5', 'lesson_6', 'lesson_7', 'lesson_8', 'lesson_9'] are not present in the metadata."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sdv.single_table import CopulaGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "patterns = {\n",
    "    \"increasing\": [\"+,+\", \"-,-\", \"+,+\"],\n",
    "    \"fluctuating\": [\"+,-\", \"-,+\", \"+,-\"],\n",
    "    \"steady\": [\"+,-\", \"-,+\"],\n",
    "    \"random\": [\"+,-\", \"-,+\"],\n",
    "}\n",
    "\n",
    "\n",
    "def generate_synthetic_data(pattern, sequence_length, num_sequences):\n",
    "    generator = CopulaGANSynthesizer(\n",
    "        metadata=SingleTableMetadata.load_from_dict(\n",
    "            {\n",
    "                \"lesson_1\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_2\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_3\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_4\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_5\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_6\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_7\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_8\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_9\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_10\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_11\": {\"sdtype\": \"categorical\"},\n",
    "                \"lesson_12\": {\"sdtype\": \"categorical\"},\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_sequences):\n",
    "        sequence = []\n",
    "        for pattern_item in patterns[pattern]:\n",
    "            sequence.extend(pattern_item.split(\",\"))\n",
    "        data.append(sequence * (sequence_length // len(sequence)))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[f\"lesson_{i+1}\" for i in range(sequence_length)])\n",
    "    generator.fit(df)\n",
    "    synthetic_data = generator.sample(num_sequences)\n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "synthetic_data_increasing = generate_synthetic_data(\"increasing\", 12, 100)\n",
    "# synthetic_data_fluctuating = generate_synthetic_data(\"fluctuating\", 16, 100)\n",
    "# synthetic_data_steady = generate_synthetic_data(\"steady\", 16, 100)\n",
    "# synthetic_data_random = generate_synthetic_data(\"random\", 16, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasha\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\datasets-generation-iKdB8DkJ-py3.12\\Lib\\site-packages\\sdv\\single_table\\base.py:82: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16\n",
       "0  +  +  +  -  -  -  +  +  -  +  +  -  -  -  -  +\n",
       "1  +  -  +  +  -  +  +  -  +  +  +  +  +  +  +  +\n",
       "2  +  +  -  +  +  -  -  +  -  +  +  +  -  -  +  +\n",
       "3  -  -  +  +  +  +  -  +  +  +  +  +  +  +  +  +\n",
       "4  +  -  +  +  +  -  +  -  -  +  +  +  +  -  -  +"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import json\n",
    "\n",
    "# write a metadata object to describe the data, primary key, etc.\n",
    "# this can be done manually or you can auto-detect/update it\n",
    "\n",
    "data = pd.DataFrame(json.load(open(\"./attendance/attendance_patterns.json\")))\n",
    "data.columns = [str(i) for i in range(1, 17)]\n",
    "data.head()\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data)\n",
    "\n",
    "model = CTGANSynthesizer(metadata)\n",
    "model.fit(data)\n",
    "\n",
    "model.save(\"./attendance/attendance.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "model: CTGANSynthesizer = CTGANSynthesizer.load(\"./attendance/attendance.pkl\")\n",
    "model.sample(100000).to_csv(\"./attendance/attendance_examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import load_config\n",
    "import pandas as pd\n",
    "\n",
    "subjects: list[str] = load_config().subjects\n",
    "\n",
    "pd.DataFrame(subjects, columns=[\"Название\"]).to_csv(\"./subjects/subjects.csv\", index_label=\"Номер\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11831    -\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "attendance_examples: pd.DataFrame = pd.read_csv(\"attendance/attendance_examples.csv\")\n",
    "sample: pd.DataFrame = attendance_examples.sample(1)\n",
    "print(sample[\"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS Code Projects\\big-data-6-sem\\datasets\\attendance\\__init__.py:101: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  used_students_ids = pd.read_csv(attendance_filename, on_bad_lines=\"warn\")[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new students for updating attendance dataset\n",
      "Counting records in attendance dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS Code Projects\\big-data-6-sem\\datasets\\attendance\\__init__.py:29: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(attendance_filename).shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendance contains 20372062 records\n"
     ]
    }
   ],
   "source": [
    "from attendance import update_attendance_dataset\n",
    "\n",
    "update_attendance_dataset(max_futures=20, max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"attendance/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arrow_converter import csv_to_parquet\n",
    "\n",
    "\n",
    "csv_to_parquet(\"attendance/attendance.csv\", \"attendance/attendance.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasets-generation-aFisT93X-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
